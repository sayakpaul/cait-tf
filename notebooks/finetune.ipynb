{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sayakpaul/cait-tf/blob/main/notebooks/finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89B27-TGiDNB"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLellw57Vlz2"
      },
      "outputs": [],
      "source": [
        "!pip install -q tensorflow-addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9u3d4Z7uQsmp"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "tfds.disable_progress_bar()\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPo10cahZXXQ"
      },
      "source": [
        "## GPUs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpvUOuC3j27n"
      },
      "outputs": [],
      "source": [
        "try:  # detect TPUs\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()  # TPU detection\n",
        "    strategy = tf.distribute.TPUStrategy(tpu)\n",
        "except ValueError:  # detect GPUs\n",
        "    tpu = False\n",
        "    strategy = (\n",
        "        tf.distribute.get_strategy()\n",
        "    )  # default strategy that works on CPU and single GPU\n",
        "print(\"Number of Accelerators: \", strategy.num_replicas_in_sync)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9S3uKC_iXY5"
      },
      "source": [
        "## Configuration\n",
        "\n",
        "Find the list of all fine-tunable models [here](https://tfhub.dev/sayakpaul/collections/cait/1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCc6tdUGnD4C"
      },
      "outputs": [],
      "source": [
        "# Model\n",
        "IMAGE_SIZE = [224, 224]\n",
        "MODEL_PATH = \"https://tfhub.dev/sayakpaul/cait_xxs24_224_fe/1\"  \n",
        "\n",
        "# TPU\n",
        "if tpu:\n",
        "    BATCH_SIZE = (\n",
        "        16 * strategy.num_replicas_in_sync\n",
        "    )  # a TPU has 8 cores so this will be 128\n",
        "else:\n",
        "    BATCH_SIZE = 128  # on Colab/GPU, a higher batch size may throw(OOM)\n",
        "\n",
        "# Dataset\n",
        "CLASSES = [\n",
        "    \"dandelion\",\n",
        "    \"daisy\",\n",
        "    \"tulips\",\n",
        "    \"sunflowers\",\n",
        "    \"roses\",\n",
        "]  # don't change the order\n",
        "\n",
        "# Other constants\n",
        "MEAN = tf.constant([0.485 * 255, 0.456 * 255, 0.406 * 255])  # imagenet mean\n",
        "STD = tf.constant([0.229 * 255, 0.224 * 255, 0.225 * 255])  # imagenet std\n",
        "AUTO = tf.data.AUTOTUNE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iTImGI5qMQT"
      },
      "source": [
        "# Data Pipeline\n",
        "\n",
        "[CaiT authors](https://arxiv.org/abs/2103.17239) use a separate preprocessing pipeline for fine-tuning. But for keeping this walkthrough short and simple, we can just perform the basic ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h29TLx7gqN_7"
      },
      "outputs": [],
      "source": [
        "def make_dataset(dataset: tf.data.Dataset, train: bool, image_size: int = IMAGE_SIZE):\n",
        "    def preprocess(image, label):\n",
        "        # for training, do augmentation\n",
        "        if train:\n",
        "            if tf.random.uniform(shape=[]) > 0.5:\n",
        "                image = tf.image.flip_left_right(image)\n",
        "        image = tf.image.resize(image, size=image_size, method=\"bicubic\")\n",
        "        image = (image - MEAN) / STD  # normalization\n",
        "        return image, label\n",
        "\n",
        "    if train:\n",
        "        dataset = dataset.shuffle(BATCH_SIZE * 10)\n",
        "\n",
        "    return dataset.map(preprocess, AUTO).batch(BATCH_SIZE).prefetch(AUTO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMQ3Qs9_pddU"
      },
      "source": [
        "# Flower Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3G-2aUBQJ-H"
      },
      "outputs": [],
      "source": [
        "train_dataset, val_dataset = tfds.load(\n",
        "    \"tf_flowers\",\n",
        "    split=[\"train[:90%]\", \"train[90%:]\"],\n",
        "    as_supervised=True,\n",
        "    try_gcs=False,  # gcs_path is necessary for tpu,\n",
        ")\n",
        "\n",
        "num_train = tf.data.experimental.cardinality(train_dataset)\n",
        "num_val = tf.data.experimental.cardinality(val_dataset)\n",
        "print(f\"Number of training examples: {num_train}\")\n",
        "print(f\"Number of validation examples: {num_val}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2X7sE3oRLXN"
      },
      "source": [
        "## Prepare dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oftrfYw1qXei"
      },
      "outputs": [],
      "source": [
        "train_dataset = make_dataset(train_dataset, True)\n",
        "val_dataset = make_dataset(val_dataset, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNyCCM6PRM8I"
      },
      "source": [
        "## Visualize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IaGzFUUVqjaC"
      },
      "outputs": [],
      "source": [
        "sample_images, sample_labels = next(iter(train_dataset))\n",
        "\n",
        "plt.figure(figsize=(5 * 3, 3 * 3))\n",
        "for n in range(15):\n",
        "    ax = plt.subplot(3, 5, n + 1)\n",
        "    image = (sample_images[n] * STD + MEAN).numpy()\n",
        "    image = (image - image.min()) / (\n",
        "        image.max() - image.min()\n",
        "    )  # convert to [0, 1] for avoiding matplotlib warning\n",
        "    plt.imshow(image)\n",
        "    plt.title(CLASSES[sample_labels[n]])\n",
        "    plt.axis(\"off\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALtRUlxhw8Vt"
      },
      "source": [
        "# Model Utility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JD9SI_Q9JdAB"
      },
      "outputs": [],
      "source": [
        "def get_model(\n",
        "    model_url: str, res: int = IMAGE_SIZE[0], num_classes: int = 5\n",
        ") -> tf.keras.Model:\n",
        "    inputs = tf.keras.Input((res, res, 3))\n",
        "    hub_module = hub.KerasLayer(model_url, trainable=True)\n",
        "\n",
        "    x, _, _ = hub_module(inputs)  # Second and third outputs in the tuple is a\n",
        "    # dictionary containing attention scores.\n",
        "    outputs = keras.layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "    return tf.keras.Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpZApp9u9_Y-"
      },
      "outputs": [],
      "source": [
        "get_model(MODEL_PATH).summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMfenMQcxAAb"
      },
      "source": [
        "# Training Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1D7Iu7oD8WzX"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "start_lr = 0.00001\n",
        "min_lr = 0.00001\n",
        "max_lr = 0.0002\n",
        "rampup_epochs = 5\n",
        "sustain_epochs = 0\n",
        "exp_decay = 0.8\n",
        "\n",
        "\n",
        "def lrfn(epoch):\n",
        "    def lr(epoch, start_lr, min_lr, max_lr, rampup_epochs, sustain_epochs, exp_decay):\n",
        "        if epoch < rampup_epochs:\n",
        "            lr = (max_lr - start_lr) / rampup_epochs * epoch + start_lr\n",
        "        elif epoch < rampup_epochs + sustain_epochs:\n",
        "            lr = max_lr\n",
        "        else:\n",
        "            lr = (max_lr - min_lr) * exp_decay ** (\n",
        "                epoch - rampup_epochs - sustain_epochs\n",
        "            ) + min_lr\n",
        "        return lr\n",
        "\n",
        "    return lr(epoch, start_lr, min_lr, max_lr, rampup_epochs, sustain_epochs, exp_decay)\n",
        "\n",
        "\n",
        "lr_callback = keras.callbacks.LearningRateScheduler(\n",
        "    lambda epoch: lrfn(epoch), verbose=True\n",
        ")\n",
        "\n",
        "rng = [i for i in range(EPOCHS)]\n",
        "y = [lrfn(x) for x in rng]\n",
        "plt.plot(rng, [lrfn(x) for x in rng])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-ID7vP5mIKs"
      },
      "outputs": [],
      "source": [
        "optimizer = tfa.optimizers.AdamW(weight_decay=1e-5)\n",
        "loss = keras.losses.SparseCategoricalCrossentropy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9p4ymNh9y7d"
      },
      "source": [
        "# Training & Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnZTSd8K90Mq"
      },
      "outputs": [],
      "source": [
        "with strategy.scope():  # this line is all that is needed to run on TPU (or multi-GPU, ...)\n",
        "    model = get_model(MODEL_PATH)\n",
        "    model.compile(loss=loss, optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "\n",
        "history = model.fit(\n",
        "    train_dataset, validation_data=val_dataset, epochs=EPOCHS, callbacks=[lr_callback]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jc7LMVz5Cbx6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "result = pd.DataFrame(history.history)\n",
        "fig, ax = plt.subplots(2, 1, figsize=(10, 10))\n",
        "result[[\"accuracy\", \"val_accuracy\"]].plot(xlabel=\"epoch\", ylabel=\"score\", ax=ax[0])\n",
        "result[[\"loss\", \"val_loss\"]].plot(xlabel=\"epoch\", ylabel=\"score\", ax=ax[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKFMWzh0Yxsq"
      },
      "source": [
        "# Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMEsR851VDZb"
      },
      "outputs": [],
      "source": [
        "sample_images, sample_labels = next(iter(val_dataset))\n",
        "\n",
        "predictions = model.predict(sample_images, batch_size=16).argmax(axis=-1)\n",
        "evaluations = model.evaluate(sample_images, sample_labels, batch_size=16)\n",
        "\n",
        "print(\"[val_loss, val_acc]\", evaluations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzCCDL1CZFx6"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(5 * 3, 3 * 3))\n",
        "for n in range(15):\n",
        "    ax = plt.subplot(3, 5, n + 1)\n",
        "    image = (sample_images[n] * STD + MEAN).numpy()\n",
        "    image = (image - image.min()) / (\n",
        "        image.max() - image.min()\n",
        "    )  # convert to [0, 1] for avoiding matplotlib warning\n",
        "    plt.imshow(image)\n",
        "    target = CLASSES[sample_labels[n]]\n",
        "    pred = CLASSES[predictions[n]]\n",
        "    plt.title(\"{} ({})\".format(target, pred))\n",
        "    plt.axis(\"off\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e5oy9zmNNID"
      },
      "source": [
        "# Reference\n",
        "* [Keras Flowers on TPU (solution)](https://colab.research.google.com/github/GoogleCloudPlatform/training-data-analyst/blob/master/courses/fast-and-lean-data-science/07_Keras_Flowers_TPU_solution.ipynb)\n",
        "\n",
        "\n",
        "This notebook is copied and modified from [here](https://github.com/sayakpaul/ConvNeXt-TF/blob/main/notebooks/finetune.ipynb). I'm thankful to [awsaf49](https://github.com/awsaf49) who originally worked on that notebook. "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "finetune",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "environment": {
      "kernel": "python3",
      "name": "tf2-gpu.2-8.m91",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m91"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
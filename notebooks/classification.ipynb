{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sayakpaul/cait-tf/blob/main/notebooks/classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zpiEiO2BUeO5",
      "metadata": {
        "id": "zpiEiO2BUeO5"
      },
      "source": [
        "# Off-the-shelf image classification with CaiT models on TF-Hub\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/sayakpaul/cait-tf/blob/main/notebooks/classification.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/sayakpaul/cait-tf/blob/main/notebooks/classification.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://tfhub.dev/sayakpaul/collections/cait/1\"><img src=\"https://www.tensorflow.org/images/hub_logo_32px.png\" />Models on Hub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "661e6538",
      "metadata": {
        "id": "661e6538"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2b73e50-6538-4af5-9878-ed99489409f5",
      "metadata": {
        "id": "f2b73e50-6538-4af5-9878-ed99489409f5"
      },
      "outputs": [],
      "source": [
        "!wget https://storage.googleapis.com/bit_models/ilsvrc2012_wordnet_lemmas.txt -O ilsvrc2012_wordnet_lemmas.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43974820-4eeb-4b3a-90b4-9ddfa00d1cb9",
      "metadata": {
        "id": "43974820-4eeb-4b3a-90b4-9ddfa00d1cb9"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow import keras\n",
        "\n",
        "\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import requests\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "z5l1cRpiSavW",
      "metadata": {
        "id": "z5l1cRpiSavW"
      },
      "source": [
        "## Select a [CaiT](https://arxiv.org/abs/2103.17239) ImageNet-1k model\n",
        "\n",
        "Find the entire collection [here] (coming soon)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0wM8idaSaOq",
      "metadata": {
        "id": "a0wM8idaSaOq"
      },
      "outputs": [],
      "source": [
        "model_name = \"cait_xxs24_224\"  # @param ['cait_xxs24_224', 'cait_xxs24_384', 'cait_xxs36_224', 'cait_xxs36_384', 'cait_xs24_384', 'cait_s24_224', 'cait_s24_384', 'cait_s36_384', 'cait_m36_384', 'cait_m48_448']\n",
        "model_handle_map = {\n",
        "    \"cait_xxs24_224\": \"https://tfhub.dev/sayakpaul/cait_xxs24_224/1\",\n",
        "    \"cait_xxs24_384\": \"https://tfhub.dev/sayakpaul/cait_xxs24_384/1\",\n",
        "    \"cait_xxs36_224\": \"https://tfhub.dev/sayakpaul/cait_xxs36_224/1\",\n",
        "    \"cait_xxs36_384\": \"https://tfhub.dev/sayakpaul/cait_xxs36_384/1\",\n",
        "    \"cait_xs24_384\": \"https://tfhub.dev/sayakpaul/cait_xs24_384/1\",\n",
        "    \"cait_s24_224\": \"https://tfhub.dev/sayakpaul/cait_s24_224/1\",\n",
        "    \"cait_s24_384\": \"https://tfhub.dev/sayakpaul/cait_s24_384/1\",\n",
        "    \"cait_s36_384\": \"https://tfhub.dev/sayakpaul/cait_s36_384/1\",\n",
        "    \"cait_m36_384\": \"https://tfhub.dev/sayakpaul/cait_m36_384/1\",\n",
        "    \"cait_m48_448\": \"https://tfhub.dev/sayakpaul/cait_m48_448/1\",\n",
        "}\n",
        "\n",
        "input_resolution = int(model_name.split(\"_\")[-1])\n",
        "model_handle = model_handle_map[model_name]\n",
        "print(f\"Input resolution: {input_resolution} x {input_resolution} x 3.\")\n",
        "print(f\"TF-Hub handle: {model_handle}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "441b5361",
      "metadata": {
        "id": "441b5361"
      },
      "source": [
        "## Image preprocessing utilities "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63e76ff1-e1e0-4c6a-91b2-4114aad60e5b",
      "metadata": {
        "id": "63e76ff1-e1e0-4c6a-91b2-4114aad60e5b"
      },
      "outputs": [],
      "source": [
        "crop_layer = keras.layers.CenterCrop(input_resolution, input_resolution)\n",
        "norm_layer = keras.layers.Normalization(\n",
        "    mean=[0.485 * 255, 0.456 * 255, 0.406 * 255],\n",
        "    variance=[(0.229 * 255) ** 2, (0.224 * 255) ** 2, (0.225 * 255) ** 2],\n",
        ")\n",
        "\n",
        "\n",
        "def preprocess_image(image, size=input_resolution):\n",
        "    image = np.array(image)\n",
        "    image_resized = tf.expand_dims(image, 0)\n",
        "    resize_size = int((256 / 224) * size)\n",
        "    image_resized = tf.image.resize(\n",
        "        image_resized, (resize_size, resize_size), method=\"bicubic\"\n",
        "    )\n",
        "    image_resized = crop_layer(image_resized)\n",
        "    return norm_layer(image_resized).numpy()\n",
        "\n",
        "\n",
        "def load_image_from_url(url):\n",
        "    # Credit: Willi Gierke\n",
        "    response = requests.get(url)\n",
        "    image = Image.open(BytesIO(response.content))\n",
        "    preprocessed_image = preprocess_image(image)\n",
        "    return image, preprocessed_image"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b961e14",
      "metadata": {
        "id": "8b961e14"
      },
      "source": [
        "## Load ImageNet-1k labels and a demo image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dc9250a-5eb6-4547-8893-dd4c746ab53b",
      "metadata": {
        "id": "8dc9250a-5eb6-4547-8893-dd4c746ab53b"
      },
      "outputs": [],
      "source": [
        "with open(\"ilsvrc2012_wordnet_lemmas.txt\", \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "imagenet_int_to_str = [line.rstrip() for line in lines]\n",
        "\n",
        "img_url = \"https://i.imgur.com/ErgfLTn.jpg\"\n",
        "image, preprocessed_image = load_image_from_url(img_url)\n",
        "\n",
        "# https://unsplash.com/photos/Ho93gVTRWW8\n",
        "plt.imshow(image)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9006a643",
      "metadata": {
        "id": "9006a643"
      },
      "source": [
        "## Run inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bHnCyJtAf9el",
      "metadata": {
        "id": "bHnCyJtAf9el"
      },
      "outputs": [],
      "source": [
        "def get_model(model_url: str) -> tf.keras.Model:\n",
        "    inputs = tf.keras.Input((input_resolution, input_resolution, 3))\n",
        "    hub_module = hub.KerasLayer(model_url)\n",
        "\n",
        "    outputs, _, _ = hub_module(\n",
        "        inputs\n",
        "    )  # Second and third outputs in the tuple is a dictionary\n",
        "    # containing attention scores.\n",
        "\n",
        "    return tf.keras.Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dfd2c7d-e454-48da-a40b-cd5d6f6c4908",
      "metadata": {
        "id": "8dfd2c7d-e454-48da-a40b-cd5d6f6c4908"
      },
      "outputs": [],
      "source": [
        "classification_model = get_model(model_handle)\n",
        "predictions = classification_model.predict(preprocessed_image)\n",
        "predicted_label = imagenet_int_to_str[int(np.argmax(predictions))]\n",
        "print(predicted_label)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TjYSHeldgS8g",
      "metadata": {
        "id": "TjYSHeldgS8g"
      },
      "source": [
        "## Obtaining attention scores\n",
        "\n",
        "The models are capable to outputting attention scores (softmax scores) for each of the transformer blocks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-duI6KRagabA",
      "metadata": {
        "id": "-duI6KRagabA"
      },
      "outputs": [],
      "source": [
        "updated_model_handle = f\"gs://tfhub-modules/sayakpaul/{model_name}/1/uncompressed\"\n",
        "\n",
        "loaded_model = tf.keras.models.load_model(updated_model_handle)\n",
        "logits, sa_atn_score_dict, ca_atn_score_dict = loaded_model.predict(preprocessed_image)\n",
        "ca_atn_score_dict.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RbnxLBBKhbib",
      "metadata": {
        "id": "RbnxLBBKhbib"
      },
      "outputs": [],
      "source": [
        "# (batch_size, nb_attention_heads, num_cls_token, seq_length)\n",
        "ca_atn_score_dict[\"ca_ffn_block_0_att\"].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AvvHi41gTBhw",
      "metadata": {
        "id": "AvvHi41gTBhw"
      },
      "source": [
        "## Visualizing attention maps - figures 6 and 7 of the [paper](https://arxiv.org/abs/2103.17239)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82826a5a",
      "metadata": {
        "id": "82826a5a"
      },
      "source": [
        "### Class attention maps (spatial-class relationship)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2d7df4f",
      "metadata": {
        "id": "e2d7df4f"
      },
      "outputs": [],
      "source": [
        "# Reference:\n",
        "# https://github.com/facebookresearch/dino/blob/main/visualize_attention.py\n",
        "\n",
        "patch_size = 16\n",
        "\n",
        "\n",
        "def get_cls_attention_map(\n",
        "    attn_score_dict=ca_atn_score_dict,\n",
        "    block_key=\"ca_ffn_block_0_att\",\n",
        "    return_saliency=False,\n",
        "):\n",
        "    w_featmap = preprocessed_image.shape[2] // patch_size\n",
        "    h_featmap = preprocessed_image.shape[1] // patch_size\n",
        "\n",
        "    attention_scores = attn_score_dict[block_key]\n",
        "    nh = attention_scores.shape[1]  # Number of attention heads.\n",
        "\n",
        "    # Taking the representations from CLS token.\n",
        "    attentions = attention_scores[0, :, 0, 1:].reshape(nh, -1)\n",
        "    print(attentions.shape)\n",
        "\n",
        "    # Reshape the attention scores to resemble mini patches.\n",
        "    attentions = attentions.reshape(nh, w_featmap, h_featmap)\n",
        "\n",
        "    if not return_saliency:\n",
        "        attentions = attentions.transpose((1, 2, 0))\n",
        "        print(attentions.shape)\n",
        "\n",
        "    else:\n",
        "        attentions = np.mean(attentions, axis=0)\n",
        "        attentions = (attentions - attentions.min()) / (\n",
        "            attentions.max() - attentions.min()\n",
        "        )\n",
        "        attentions = np.expand_dims(attentions, -1)\n",
        "        print(attentions.shape)\n",
        "\n",
        "    # Resize the attention patches to 224x224 (224: 14x16)\n",
        "    attentions = tf.image.resize(\n",
        "        attentions,\n",
        "        size=(h_featmap * patch_size, w_featmap * patch_size),\n",
        "        method=\"bicubic\",\n",
        "    )\n",
        "    print(attentions.shape)\n",
        "\n",
        "    return attentions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "012ad859",
      "metadata": {
        "id": "012ad859"
      },
      "outputs": [],
      "source": [
        "attentions_ca_block_0 = get_cls_attention_map()\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(13, 13))\n",
        "img_count = 0\n",
        "\n",
        "for i in range(attentions_ca_block_0.shape[-1]):\n",
        "    if img_count < attentions_ca_block_0.shape[-1]:\n",
        "        axes[i].imshow(attentions_ca_block_0[:, :, img_count])\n",
        "        axes[i].title.set_text(f\"Attention head: {img_count}\")\n",
        "        axes[i].axis(\"off\")\n",
        "        img_count += 1\n",
        "\n",
        "fig.tight_layout()\n",
        "fig.savefig(\"class_attention_heads_0.png\", dpi=300, bbox_inches=\"tight\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7519cef4",
      "metadata": {
        "id": "7519cef4"
      },
      "outputs": [],
      "source": [
        "attentions_ca_block_1 = get_cls_attention_map(block_key=\"ca_ffn_block_1_att\")\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(13, 13))\n",
        "img_count = 0\n",
        "\n",
        "for i in range(attentions_ca_block_1.shape[-1]):\n",
        "    if img_count < attentions_ca_block_1.shape[-1]:\n",
        "        axes[i].imshow(attentions_ca_block_1[:, :, img_count])\n",
        "        axes[i].title.set_text(f\"Attention head: {img_count}\")\n",
        "        axes[i].axis(\"off\")\n",
        "        img_count += 1\n",
        "\n",
        "fig.tight_layout()\n",
        "fig.savefig(\"class_attention_heads_1.png\", dpi=300, bbox_inches=\"tight\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "292e5549",
      "metadata": {
        "id": "292e5549"
      },
      "source": [
        "### Saliency maps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "baeabd80",
      "metadata": {
        "id": "baeabd80"
      },
      "outputs": [],
      "source": [
        "saliency_attention = get_cls_attention_map(return_saliency=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b57be69",
      "metadata": {
        "id": "8b57be69"
      },
      "outputs": [],
      "source": [
        "image = np.array(image)\n",
        "image_resized = tf.expand_dims(image, 0)\n",
        "resize_size = int((256 / 224) * input_resolution)\n",
        "image_resized = tf.image.resize(\n",
        "    image_resized, (resize_size, resize_size), method=\"bicubic\"\n",
        ")\n",
        "image_resized = crop_layer(image_resized)\n",
        "\n",
        "plt.imshow(image_resized.numpy().squeeze().astype(\"int32\"))\n",
        "plt.imshow(saliency_attention.numpy().squeeze(), cmap=\"cividis\", alpha=0.9)\n",
        "plt.axis(\"off\")\n",
        "plt.savefig(\"saliency_attention_map.png\", dpi=300, bbox_inches=\"tight\")\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "name": "classification.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "environment": {
      "kernel": "python3",
      "name": "tf2-gpu.2-7.m84",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-7:m84"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}